wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yashjain14 (yashjain14-nanyang-technological-university-singapore) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 409, in hf_raise_for_status
[rank0]:     response.raise_for_status()
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/requests/models.py", line 1024, in raise_for_status
[rank0]:     raise HTTPError(http_error_msg, response=self)
[rank0]: requests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1484, in _get_metadata_or_catch_error
[rank0]:     metadata = get_hf_file_metadata(
[rank0]:                ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1401, in get_hf_file_metadata
[rank0]:     r = _request_wrapper(
[rank0]:         ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 285, in _request_wrapper
[rank0]:     response = _request_wrapper(
[rank0]:                ^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 309, in _request_wrapper
[rank0]:     hf_raise_for_status(response)
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 473, in hf_raise_for_status
[rank0]:     raise _format(HfHubHTTPError, message, response) from e
[rank0]: huggingface_hub.errors.HfHubHTTPError: (Request ID: Root=1-67f36f91-1605afbc5b6228d717a6f03c;5efd6d4d-9996-4727-88c0-1c1e33d28cfd)

[rank0]: 403 Forbidden: Please enable access to public gated repositories in your fine-grained token settings to view this repository..
[rank0]: Cannot access content at: https://huggingface.co/google/gemma-3-1b-it/resolve/main/config.json.
[rank0]: Make sure your token has the correct permissions.

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/utils/hub.py", line 424, in cached_files
[rank0]:     hf_hub_download(
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
[rank0]:     return fn(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 961, in hf_hub_download
[rank0]:     return _hf_hub_download_to_cache_dir(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1068, in _hf_hub_download_to_cache_dir
[rank0]:     _raise_on_head_call_error(head_call_error, force_download, local_files_only)
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1599, in _raise_on_head_call_error
[rank0]:     raise LocalEntryNotFoundError(
[rank0]: huggingface_hub.errors.LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.

[rank0]: The above exception was the direct cause of the following exception:

[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/UG/yash012/Gemma_TSA/pipeline/compare_transformers.py", line 141, in <module>
[rank0]:     main()
[rank0]:   File "/home/UG/yash012/Gemma_TSA/pipeline/compare_transformers.py", line 66, in main
[rank0]:     model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 531, in from_pretrained
[rank0]:     config, kwargs = AutoConfig.from_pretrained(
[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py", line 1112, in from_pretrained
[rank0]:     config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
[rank0]:                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 590, in get_config_dict
[rank0]:     config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/configuration_utils.py", line 649, in _get_config_dict
[rank0]:     resolved_config_file = cached_file(
[rank0]:                            ^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/utils/hub.py", line 266, in cached_file
[rank0]:     file = cached_files(path_or_repo_id=path_or_repo_id, filenames=[filename], **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/utils/hub.py", line 491, in cached_files
[rank0]:     raise OSError(
[rank0]: OSError: We couldn't connect to 'https://huggingface.co' to load the files, and couldn't find them in the cached files.
[rank0]: Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.
[rank0]:[W407 14:24:18.720400593 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
