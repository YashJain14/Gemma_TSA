wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: yashjain14 (yashjain14-nanyang-technological-university-singapore) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]Generating train split:   4%|▍         | 1000/25000 [00:00<00:08, 2908.97 examples/s]Generating train split: 100%|██████████| 25000/25000 [00:00<00:00, 37438.19 examples/s]
Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]Generating test split:  60%|██████    | 15000/25000 [00:00<00:00, 115612.87 examples/s]Generating test split: 100%|██████████| 25000/25000 [00:00<00:00, 61954.85 examples/s] 
Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]Generating unsupervised split:   2%|▏         | 1000/50000 [00:00<00:05, 8785.81 examples/s]Generating unsupervised split:  60%|██████    | 30000/50000 [00:00<00:00, 159898.79 examples/s]Generating unsupervised split: 100%|██████████| 50000/50000 [00:00<00:00, 61390.02 examples/s] 
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Map:   4%|▍         | 1000/25000 [00:00<00:16, 1440.50 examples/s]Map:   8%|▊         | 2000/25000 [00:01<00:15, 1527.21 examples/s]Map:  12%|█▏        | 3000/25000 [00:01<00:14, 1534.62 examples/s]Map:  16%|█▌        | 4000/25000 [00:02<00:13, 1533.00 examples/s]Map:  20%|██        | 5000/25000 [00:03<00:13, 1513.68 examples/s]Map:  24%|██▍       | 6000/25000 [00:03<00:12, 1513.61 examples/s]Map:  28%|██▊       | 7000/25000 [00:04<00:11, 1519.88 examples/s]Map:  32%|███▏      | 8000/25000 [00:05<00:11, 1506.34 examples/s]Map:  36%|███▌      | 9000/25000 [00:05<00:10, 1501.40 examples/s]Map:  40%|████      | 10000/25000 [00:06<00:09, 1524.10 examples/s]Map:  44%|████▍     | 11000/25000 [00:07<00:09, 1548.17 examples/s]Map:  48%|████▊     | 12000/25000 [00:07<00:08, 1542.26 examples/s]Map:  52%|█████▏    | 13000/25000 [00:08<00:07, 1544.63 examples/s]Map:  56%|█████▌    | 14000/25000 [00:09<00:07, 1499.88 examples/s]Map:  60%|██████    | 15000/25000 [00:09<00:06, 1504.03 examples/s]Map:  64%|██████▍   | 16000/25000 [00:10<00:06, 1493.08 examples/s]Map:  68%|██████▊   | 17000/25000 [00:11<00:05, 1480.83 examples/s]Map:  72%|███████▏  | 18000/25000 [00:11<00:04, 1492.01 examples/s]Map:  76%|███████▌  | 19000/25000 [00:12<00:03, 1504.53 examples/s]Map:  80%|████████  | 20000/25000 [00:13<00:03, 1482.51 examples/s]Map:  84%|████████▍ | 21000/25000 [00:13<00:02, 1485.76 examples/s]Map:  88%|████████▊ | 22000/25000 [00:14<00:02, 1471.57 examples/s]Map:  92%|█████████▏| 23000/25000 [00:15<00:01, 1494.33 examples/s]Map:  96%|█████████▌| 24000/25000 [00:15<00:00, 1485.80 examples/s]Map: 100%|██████████| 25000/25000 [00:16<00:00, 1481.59 examples/s]Map: 100%|██████████| 25000/25000 [00:17<00:00, 1437.86 examples/s]
Map:   0%|          | 0/25000 [00:00<?, ? examples/s]Map:   4%|▍         | 1000/25000 [00:00<00:15, 1548.66 examples/s]Map:   8%|▊         | 2000/25000 [00:01<00:14, 1571.45 examples/s]Map:  12%|█▏        | 3000/25000 [00:01<00:14, 1552.42 examples/s]Map:  16%|█▌        | 4000/25000 [00:02<00:13, 1570.71 examples/s]Map:  20%|██        | 5000/25000 [00:03<00:12, 1558.16 examples/s]Map:  24%|██▍       | 6000/25000 [00:04<00:13, 1368.15 examples/s]Map:  28%|██▊       | 7000/25000 [00:04<00:12, 1441.04 examples/s]Map:  32%|███▏      | 8000/25000 [00:05<00:11, 1465.55 examples/s]Map:  36%|███▌      | 9000/25000 [00:06<00:10, 1465.97 examples/s]Map:  40%|████      | 10000/25000 [00:06<00:10, 1482.70 examples/s]Map:  44%|████▍     | 11000/25000 [00:07<00:09, 1510.15 examples/s]Map:  48%|████▊     | 12000/25000 [00:08<00:08, 1520.56 examples/s]Map:  52%|█████▏    | 13000/25000 [00:08<00:07, 1521.40 examples/s]Map:  56%|█████▌    | 14000/25000 [00:09<00:07, 1525.01 examples/s]Map:  60%|██████    | 15000/25000 [00:09<00:06, 1519.11 examples/s]Map:  64%|██████▍   | 16000/25000 [00:10<00:05, 1511.40 examples/s]Map:  68%|██████▊   | 17000/25000 [00:11<00:05, 1511.38 examples/s]Map:  72%|███████▏  | 18000/25000 [00:11<00:04, 1498.61 examples/s]Map:  76%|███████▌  | 19000/25000 [00:12<00:03, 1512.00 examples/s]Map:  80%|████████  | 20000/25000 [00:13<00:03, 1526.38 examples/s]Map:  84%|████████▍ | 21000/25000 [00:13<00:02, 1543.93 examples/s]Map:  88%|████████▊ | 22000/25000 [00:14<00:01, 1514.94 examples/s]Map:  92%|█████████▏| 23000/25000 [00:15<00:01, 1505.34 examples/s]Map:  96%|█████████▌| 24000/25000 [00:15<00:00, 1517.90 examples/s]Map: 100%|██████████| 25000/25000 [00:16<00:00, 1528.51 examples/s]Map: 100%|██████████| 25000/25000 [00:17<00:00, 1468.04 examples/s]
Map:   0%|          | 0/50000 [00:00<?, ? examples/s]Map:   2%|▏         | 1000/50000 [00:00<00:31, 1575.27 examples/s]Map:   4%|▍         | 2000/50000 [00:01<00:31, 1519.54 examples/s]Map:   6%|▌         | 3000/50000 [00:02<00:31, 1483.23 examples/s]Map:   8%|▊         | 4000/50000 [00:02<00:30, 1485.43 examples/s]Map:  10%|█         | 5000/50000 [00:03<00:30, 1473.89 examples/s]Map:  12%|█▏        | 6000/50000 [00:03<00:29, 1509.66 examples/s]Map:  14%|█▍        | 7000/50000 [00:04<00:28, 1497.79 examples/s]Map:  16%|█▌        | 8000/50000 [00:05<00:28, 1489.31 examples/s]Map:  18%|█▊        | 9000/50000 [00:05<00:27, 1508.13 examples/s]Map:  20%|██        | 10000/50000 [00:06<00:27, 1481.03 examples/s]Map:  22%|██▏       | 11000/50000 [00:07<00:26, 1488.32 examples/s]Map:  24%|██▍       | 12000/50000 [00:08<00:25, 1500.69 examples/s]Map:  26%|██▌       | 13000/50000 [00:08<00:24, 1510.08 examples/s]Map:  28%|██▊       | 14000/50000 [00:09<00:24, 1490.51 examples/s]Map:  30%|███       | 15000/50000 [00:10<00:25, 1359.79 examples/s]Map:  32%|███▏      | 16000/50000 [00:10<00:24, 1391.56 examples/s]Map:  34%|███▍      | 17000/50000 [00:11<00:23, 1413.72 examples/s]Map:  36%|███▌      | 18000/50000 [00:12<00:22, 1430.29 examples/s]Map:  38%|███▊      | 19000/50000 [00:12<00:21, 1438.00 examples/s]Map:  40%|████      | 20000/50000 [00:13<00:20, 1437.51 examples/s]Map:  42%|████▏     | 21000/50000 [00:14<00:19, 1461.17 examples/s]Map:  44%|████▍     | 22000/50000 [00:15<00:19, 1467.88 examples/s]Map:  46%|████▌     | 23000/50000 [00:15<00:18, 1468.58 examples/s]Map:  48%|████▊     | 24000/50000 [00:16<00:17, 1481.84 examples/s]Map:  50%|█████     | 25000/50000 [00:17<00:16, 1483.41 examples/s]Map:  52%|█████▏    | 26000/50000 [00:17<00:16, 1496.12 examples/s]Map:  54%|█████▍    | 27000/50000 [00:18<00:15, 1490.63 examples/s]Map:  56%|█████▌    | 28000/50000 [00:18<00:14, 1526.35 examples/s]Map:  58%|█████▊    | 29000/50000 [00:19<00:14, 1493.48 examples/s]Map:  60%|██████    | 30000/50000 [00:20<00:13, 1507.14 examples/s]Map:  62%|██████▏   | 31000/50000 [00:21<00:12, 1480.73 examples/s]Map:  64%|██████▍   | 32000/50000 [00:21<00:12, 1483.20 examples/s]Map:  66%|██████▌   | 33000/50000 [00:22<00:11, 1482.80 examples/s]Map:  68%|██████▊   | 34000/50000 [00:23<00:10, 1501.23 examples/s]Map:  70%|███████   | 35000/50000 [00:23<00:10, 1434.21 examples/s]Map:  72%|███████▏  | 36000/50000 [00:24<00:09, 1458.00 examples/s]Map:  74%|███████▍  | 37000/50000 [00:25<00:08, 1477.98 examples/s]Map:  76%|███████▌  | 38000/50000 [00:25<00:08, 1493.98 examples/s]Map:  78%|███████▊  | 39000/50000 [00:26<00:07, 1511.67 examples/s]Map:  80%|████████  | 40000/50000 [00:27<00:06, 1514.32 examples/s]Map:  82%|████████▏ | 41000/50000 [00:27<00:06, 1494.71 examples/s]Map:  84%|████████▍ | 42000/50000 [00:28<00:05, 1503.16 examples/s]Map:  86%|████████▌ | 43000/50000 [00:29<00:04, 1503.44 examples/s]Map:  88%|████████▊ | 44000/50000 [00:29<00:04, 1495.35 examples/s]Map:  90%|█████████ | 45000/50000 [00:30<00:03, 1507.79 examples/s]Map:  92%|█████████▏| 46000/50000 [00:31<00:02, 1497.36 examples/s]Map:  94%|█████████▍| 47000/50000 [00:31<00:02, 1490.13 examples/s]Map:  96%|█████████▌| 48000/50000 [00:32<00:01, 1368.84 examples/s]Map:  98%|█████████▊| 49000/50000 [00:33<00:00, 1411.22 examples/s]Map: 100%|██████████| 50000/50000 [00:33<00:00, 1441.63 examples/s]Map: 100%|██████████| 50000/50000 [00:34<00:00, 1444.41 examples/s]
/home/UG/yash012/Gemma_TSA/utils/trainer.py:148: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.
  super().__init__(*args,
wandb: WARNING Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to https://wandb.me/wandb-init.
wandb: Tracking run with wandb version 0.19.9
wandb: Run data is saved locally in /home/UG/yash012/Gemma_TSA/wandb/run-20250406_232350-zqmdungy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run roberta-CompareTransformers-imdb
wandb: ⭐️ View project at https://wandb.ai/yashjain14-nanyang-technological-university-singapore/huggingface
wandb: 🚀 View run at https://wandb.ai/yashjain14-nanyang-technological-university-singapore/huggingface/runs/zqmdungy
  0%|          | 0/1875 [00:00<?, ?it/s][rank0]: Traceback (most recent call last):
[rank0]:   File "/home/UG/yash012/Gemma_TSA/pipeline/compare_transformers.py", line 141, in <module>
[rank0]:     main()
[rank0]:   File "/home/UG/yash012/Gemma_TSA/pipeline/compare_transformers.py", line 126, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/UG/yash012/.conda/envs/llm_env/lib/python3.12/site-packages/transformers/trainer.py", line 3736, in training_step
[rank0]:     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: TypeError: CustomTrainer.compute_loss() got an unexpected keyword argument 'num_items_in_batch'
